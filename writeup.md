# Behavioral Cloning Project

The goals / steps of this project are the following:
* Use the simulator to collect data of good driving behavior
* Build, a convolution neural network in Keras that predicts steering angles from images
* Train and validate the model with a training and validation set
* Test that the model successfully drives around track one without leaving the road
* Summarize the results with a written report


[//]: # (Image References)

[image1]: ./images/transformations.png "Transformations"
[image2]: ./images/cropped_image.jpg "Cropped"
[video1]: ./comparison.mp4 "Comparison"
## Rubric Points
### Here I will consider the [rubric points](https://review.udacity.com/#!/rubrics/432/view) individually and describe how I addressed each point in my implementation.  

---
### Files Submitted & Code Quality

#### 1. Submission includes all required files and can be used to run the simulator in autonomous mode

My project includes the following files:
* model.py containing the script to create and train the model
* generate.py for generating new pictures from the original training set
* batch_clone_continue.py to be able to continue training from an already pretrained model
* drive.py for driving the car in autonomous mode
* model.h5 containing a trained convolution neural network
* writeup_report.md summarizing the results
* video.mp4 is the video of circuit 1
* video_2.mp4 is the video of the circuit 2
* comparison.mp4 is a side by side comprarison of circuit 2 video with and without recording, same model.


#### 2. Submission includes functional code

Using the Udacity provided simulator and my drive.py file, the car can be driven autonomously around the track by executing
```sh
python drive.py model.h5
```
There are some subtle points regarding the autonomous driving and drive.py :
* drive.py uses a simple PI controller for speed
* Sometimes the simulator or drive.py stops the car needing a "S" or backward key to restart it. That's specially true with low speeds.
* The I of the PI continues to add throttle value so the car restarts at high speed.
* When recording video (at least in my MacBook pro) things slow so the driving is less smooth. In track 2 I needed to stop some background processes like Dropbox or Time Machine to be able to get a correct driving video.
* Due to this effect recorded videos have much more oscilations than the not recorded ones.

Explanation is simple. It seems that when the simulating loop is slower the controller is called less frequently so it is not able to adjust the steering angle as fast as it needs.

That is no problem in track 1 as the "gain" may be reduced but is more problematic in track 2 because to negotiate the first left turn or other demanding features it needs high gain.

So although without recording the model is able to complete the second track at 13 km/h, when recording, speed must be reduced to 8 km/h and then sometimes the car stops and must be restarted carefully (as explained) with the S.

The video ["comparison.mp4"](./comparison.mp4) shows a comparison with and without recording. Of course screen recording also slows processes so the effect is magnified.


#### 3. Submission code is usable and readable

The model.py file contains the code for training and saving the convolution neural network. The file shows the pipeline I used for training and validating the model, and it contains comments to explain how the code works.

Aditional programs are ** generate.py ** which generates new data and ** batch_clone_continue.py ** that refines training from a previous model.

### Model Architecture and Training Strategy

#### 1. An appropriate model architecture has been employed

The model used is the same in Nvidia paper.

The model includes RELU layers to introduce nonlinearity in every convolutional and fully connected layer, and the data is normalized in the model using a Keras lambda layer (code line 106).

Cropping is used to eliminate the sky and the bottom of the image and 24 pixels each side to compensate the black areas due to the image generation procedures.



#### 2. Attempts to reduce overfitting in the model

The model contains dropout layers in order to reduce overfitting between the fully connected layers.

The model was trained and validated on different data sets 80/20% of the total data. The sets were generated by the buildDirectory procedure at line 22.

No big overfitting was observed. Probably 15 epochs was too low to show it. Also it used data augmentation to reduce overfitting and get better steering values.

#### 3. Model parameter tuning

The model used an adam optimizer, so the learning rate was not tuned manually.

When training in the first track it needed at least 3 turns of training data which amounts to 12828 images, to be able to finish it correctly using left and right cameras with a correction coefficient of 0.2.

Unfortunately the same model when trained and tested against the second track performed poorly.

Second track was challenging.

I augmented data, included special train sections but the first left closed turn was a definitely impossible turn.

Data augmentation is explained in a appropriate section.

Finally, I checked the manual steering values against the ones generated by the model with the same images and they were between 1/2 and 1/3 of those generated manually with the ** Show_Drive.py ** program.

As the training central camera generated "near center data" most of the correction values comes from the lateral cameras, so the 0.2 correction, which in some way defined the "gain" of the loop was clearly too low.

Once increased to 6 the model was perfectly capable of negotiating track 2 most of the time.

Finally, 7 was a bit too high with wobbling so a 6.5 values was chosen as  satisfactory and without recording is able to negotiate all track 2 many times and at 13 km/h speeds.


#### 4. Appropriate training data

Training data was chosen to keep the vehicle driving on the road. I used a combination of center lane driving, recovering from the left and right sides of the road ...

Also in some trains data was augmented by generating new images/steering values from the recorded ones.

For details about how I created the training data, see the next section.

### Model Architecture and Training Strategy

#### 1. Solution Design Approach

By unknown reasons I jumped over some of the project lessons so I directly implemented the [NVidia model](https://arxiv.org/pdf/1604.07316v1.pdf)  without generators.

To make it work in the first track I needed 3 turns of data, that's about 13000 images.

Unfortunately the model was a total failure in compromised sections of track 2 and a lot of time was spent generating new data and trying to chosse new parameters and implementing the generator process.

#### 2. Final Model Architecture

The final model architecture (model.py lines 105-120) consisted of a convolution neural network with the following layers and layer sizes

| Layer         		|     Description	        					|
|:---------------------:|:---------------------------------------------:|
| Input         		| 160x320x3 image   							|
| lambda                | Normalization image values                          |
| Cropping           	| 60,25 top/bottom, 24,24 left/right         	|
| Convolution 5x5     	| 2x2 stride, valid padding, relu  24 filters   |
| Convolution 5x5     	| 2x2 stride, valid padding, relu  36 filters   |
| Convolution 5x5     	| 2x2 stride, valid padding, relu  48 filters   |
| Convolution 3x3     	| 1x1 stride, valid padding, relu  64 filters   |
| Convolution 3x3     	| 1x1 stride, valid padding, relu  64 filters   |
| Flatten				| 								                |
| Fully connected       | Outputs 100, relu                             |
| dropout               | Keep Probability 0.5                          |
| Fully connected       | Outputs 50, relu                              |
| Fully connected       | Outputs 10, relu                              |
| dropout               | Keep Probability 0.5                          |
| Fully connected       | Outputs 1, relu                               |

The cropping layer is used to remove sky and bottom car parts and also to remove the sides which are rendered unusable due to the data augmentation procedure.

Due to this generation process images should be cropped in the sides to remove the generated black areas.

![Cropped Image][image2]



#### 3. Creation of the Training Set & Training Process

Images from the 3 cameras were used. left and right camera steering values were corrected by a **correction**. As we will see this value is the critical parameter of the model.

In a real environment it may be computed from geometric constraints but here I should estimate it. Beginning by a suggestion that it was about 0.2.

First I recorded 1 turn of first track. It was clearly insufficient at places like
the bridge.

Two more turns were enough to get a robust enough model. No need to record special
cases as recovering.

All data was split 80/20 between train and validation sets.

When trying the model in track 2 it was clearly insufficient. I begun recording a track 2 turn. The model generated was capable of navigating track 1 but using the white borders and not the road. It was incapable of passing some points in track 2.

I continued to record more data from track 2. Not always easy and with no success.

Finally begun to record data in conflictive segments and record recovery situations without success.

I generated new data by shifting and turning the car. To do it we consider horizon to be approximately at pixel 60 from the top.

Here there is an image of the transformations applied to the center camera image with a value of -60 that should mimic the left and right camera (more or less).

![alt text][image1]

In the program values are between +/- 30 so the black areas are eliminated with the cropping.


In the end I was able to get 130752 training images and 32688 validation images.

Results continued to be deceptive.

When analyzed the steering values predicted in closed turns , they were much smaller than the training ones. The reason was the small correction coefficient used to modify the steering value of left and right camera images.

This coefficient acts as a **gain** in a classic closed loop system. As it was too low everything worked except in closed turns as it didn't generate a big enough steering value.

I finally increased it to about 0.65 getting good results. Without recording I may do as many turns of track 2 as I want at speeds of about 13.

When tested again with track 1 it showed some wobbling but worked correctly till 17 km/h.

Here we describe the NVidia inspired image transformations. They are implemented in the *generate.py* program.

#### 3.1 Shifting

We apply a remap that lets all pixels over the horizon unmodified and apply a displacement to the rest proportional to the distance from the horizon in a way that displacement at the horizon is 0 and at the bottom is maximum.

Of course there are some lateral areas that may not me covered. If maximum displacement is about 30 pixels these areas are 24 pixels width and are cropped.

Steering value = original steering +/- (maximum displacement / 60) * correction for left and right images.

#### 3.2 Turning

In a way is the opposite transformation. Points over the horizon are displaced and the rest are displaced maximum at horizon and 0 at bottom.

Steering correction should be proportional to maximum displacement but as this transformation has not been used it has not been calibrated.

### Aditional experiment

As we don't have a track 3 to test the driving I used track 2 in opossite direction. Results are mixed and need more work to get conclusions.

### Conclusions

This project has been great fun and although initial requirements were easily covered due to previous study of the NVidia project, getting it working in the track 2 was much more challenging but worth the effort because it allowed me to get some important insights:

* It is very important the method and values of data augmentation. If it is not done correctly we get erroneous results. GiGo at its best.

* In the method used,  the parameter to get the correct steering for the left and right camera images acted as a **gain** in a classic closed loop system. So if it is too low, the system is incapable of turning closed curves as it needs to much time to correct and if it is too high it has a tendency to oscillate (wobble).

* In a real situation this parameter may be computed from geometric considerations that were lacking in the simulator. Anyway, it is quit amazing that the system may learn without a clear understanding of what the steering value means.

* When recording in the simulator wobbling augmented and sometimes the speed should be reduced to be able to do all the second circuit due to low system power and probably disk access. That reason may be translated to the speed of reaction or sampling and acting frequency in a real system. If it is too low it will wobble a lot and will not be able to drive smooth and correctly or speeds should be much slower.

* Will be very interesting to add the speed to the network and use it as a modulation to the desired speed, at least as an experiment.

* Would have been very very interesting to get circuit 3. We would not test in circuit 3 but run the model there. Would work as a real test of the generalization of the circuit.
